# -*- coding: utf-8 -*-
"""LORA

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JXDqjUAZpHe8C-8r1WnEA_WeLpTfC41a
"""

!nvidia-smi

!pip install torch datasets transformers peft

!pip install bitsandbytes trl

import torch
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments
from peft import LoraConfig, get_peft_model, TaskType
from trl import SFTTrainer

model_name = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_quant_compute_dtype=torch.bfloat16)
model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, use_cache=False, device_map='auto', trust_remote_code=True)

tokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True)
model

lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=8,
    lora_alpha=32,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.5,
    bias='none')

model = get_peft_model(model, lora_config)

data=load_dataset('openai/gsm8k','main',split='train[:800]')
print(data)

def tokenize(batch):
  texts=[
      f"### Instruction:\n{instruction}\n### Response:\n{out}"
      for instruction, out in zip(batch['question'],batch['answer'])
  ]
  token=tokenizer(texts,padding='max_length',max_length=256,truncation=True, return_tensors='pt')
  token['label']=token['input_ids'].clone()
  return token

tokenize_data = data.map(tokenize, batched=True, remove_columns=data.column_names)

training_args = TrainingArguments(
    output_dir='./results',
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=1e-3,
    num_train_epochs=5,
    bf16=True,  # Changed from fp16=True to bf16=True
    save_total_limit=3,
    logging_steps=20,
    optim='paged_adamw_8bit',
    save_strategy='epoch',
    push_to_hub=False,
    report_to='none',
    remove_unused_columns=False,
    label_names=['labels']
)

trainer = SFTTrainer(
    model = model,
    args=training_args,
    train_dataset=tokenize_data,
    processing_class = tokenizer
)

trainer.train()

model.save_pretrained('./results-adapter')
tokenizer.save_pretrained('./results-adapter')

"""# Eval"""

import os
import math

import torch
from torch.utils.data import DataLoader

from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, default_data_collator

from peft import PeftModel

model_name = 'TinyLLama/TinyLlama-1.1B-Chat-v1.0'
adapter_path = '/content/results-adapter'

bnb_config = BitsAndBytesConfig(
    load_in_4bit = True,
    bnb_4bit_quant_type = 'nf4',
    bnb_4bit_compute_dtype = torch.bfloat16
)

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config = bnb_config,
    device_map = 'auto',
    trust_remote_code = True
).eval()

tmp_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config = bnb_config,
    device_map = 'auto',
    trust_remote_code = True
)

tuned_model = PeftModel.from_pretrained(tmp_model, adapter_path)
tuned_model = tuned_model.merge_and_unload().eval()

def tokenize(batch):
    texts = [
        f"### Instruction:\n{inst}\n### Response:\n{out}"
        for inst, out in zip(batch['question'], batch['answer'])
    ]

    tokens = tokenizer(
        texts,
        padding = 'max_length',
        truncation = True,
        max_length = 256,
        return_tensors = 'pt'
    )

    tokens['labels'] = tokens['input_ids'].clone()

    return tokens

eval_ds = load_dataset('openai/gsm8k', 'main', split='train[800:820]')
eval_ds = eval_ds.map(tokenize, batched=True, remove_columns=['question', 'answer'])
eval_ds = eval_ds.with_format('torch')

eval_loader = DataLoader(
    eval_ds,
    batch_size = 8,
    collate_fn = default_data_collator
)

@torch.no_grad()
def compute_perplexity(model):
    losses = []

    for batch in eval_loader:
        batch = {k: v.to('cuda') for k, v in batch.items()}
        loss = model(**batch).loss
        losses.append(loss.item())

    return math.exp(sum(losses) / len(losses))

print(f'Base Model Perplexity: {compute_perplexity(base_model):.2f}')
print(f'Tuned Model Perplexity: {compute_perplexity(tuned_model):.2f}')

import random

raw_data = load_dataset('gsm8k', 'main', split='train[800:820]')
refs = raw_data['answer']


def generate(model, instruction):
    token_ids = tokenizer(f'### Instruction:\n{instruction}\n### Response:\n', return_tensors='pt').input_ids.to('cuda')

    with torch.no_grad():
        out = model.generate(token_ids, max_new_tokens=256)

    #return tokenizer.decode(out[0], skip_special_tokens=True).split('### Response:\n')[-1].strip()
    return tokenizer.decode(out[0], skip_special_tokens=True)

raw_data['question'][1]

print(generate(base_model, raw_data['question'][1]))

print(generate(tuned_model, raw_data['question'][1]))

